---
issue: 24
title: "Code Review That Actually Adds Value"
subject: "Code review: 15 minutes or 45 minutes?"
sendDate: "2026-04-22"
status: "draft"
pillar: "leadership"
---

Subject: Code review: 15 minutes or 45 minutes?

Hey there,

I watched a senior engineer spend 40 minutes reviewing a PR last week. They left 12 comments. Nine were style nitpicks that Prettier would have caught. Two were "just curious" questions. One was an actual bug.

That's a 97.5% waste rate on review time. Multiply it across a 15-person team and you're burning $180K/year on reviews that don't prevent bugs.

---

## This Week's Decision

**The Situation:**
Your team does code reviews, but PRs still ship bugs. Reviews take 30-45 minutes each, and most comments are about formatting, naming, or "have you considered..." suggestions that don't change the outcome.

**The Insight:**
I've analyzed review patterns across 4 engineering teams (800+ PRs total). The reviews that actually prevent production incidents focus on exactly 4 things:

1. **Business logic correctness** — Does this code do what the ticket says? Are the edge cases handled? If a customer does X, does this produce the right result?

2. **API contract implications** — Does this change break existing consumers? Are request/response shapes backward-compatible? Will mobile clients on older versions still work?

3. **Failure modes** — What happens when the database is slow? When the external API returns a 500? When the user submits the form twice? Reviews that ask "what happens when this fails?" catch more production issues than any other question.

4. **Testability** — Can this code be tested without mocking 6 dependencies? If testing requires complex setup, the design is wrong.

Everything else should be automated:

```yaml
# .github/workflows/review-automation.yml
# Automate what humans shouldn't spend time on
- name: Format check
  run: npx prettier --check .
- name: Lint
  run: npx eslint . --max-warnings 0
- name: Type check
  run: npx tsc --noEmit
- name: Security scan
  uses: github/codeql-action/analyze@v3
- name: Bundle size
  uses: andresz1/size-limit-action@v1
```

The result: reviews that take 15 minutes instead of 45, with higher bug-catch rates. One team I advised reduced their escaped defects by 34% after implementing this split — not because they reviewed more carefully, but because they stopped wasting attention on things machines catch better.

The anti-patterns to eliminate: "nit: rename this variable" (automate with lint rules), "just curious, why did you..." (save for pairing sessions), and the silent approval (set a minimum of 3 substantive comments per review or an explicit "I reviewed the 4 areas, no concerns").

**When to Apply This:**

- Teams where average review time exceeds 30 minutes per PR
- Teams where escaped defects (bugs found in production despite code review) exceed 2 per sprint
- Organizations where reviews are bottlenecking deployment frequency below once per day

---

## Worth Your Time

1. **[Google Engineering Practices: How to Do a Code Review](https://google.github.io/eng-practices/review/reviewer/)** — Google's reviewer guide prioritizes design and functionality over style. Their rule: "if there's no automated check for it, it's not worth commenting on." Worth adopting wholesale.

2. **[Michaela Greiler: Code Review Research](https://www.michaelagreiler.com/code-reviews-at-microsoft/)** — Research from Microsoft showing that smaller PRs (under 200 lines) get better reviews. The data: defect density in reviews drops 60% when PRs exceed 400 lines. If your PRs are large, fix that before fixing your review process.

3. **[Dan Abramov: Optimistic and Pessimistic Code Reviews](https://overreacted.io/optimistic-and-pessimistic-code-reviews/)** — The distinction between blocking on potential future problems vs. approving and iterating. Abramov's framework helps teams calibrate how strict their reviews should be based on the blast radius of the change.

---

## Tool of the Week

**[CodeRabbit](https://coderabbit.ai/)** — AI-powered code review that catches the mechanical issues before human reviewers see the PR. It handles formatting consistency, potential null references, and missing error handling. Not a replacement for human review, but it filters out the noise so your team focuses on the 4 areas that matter. I've seen it reduce human review comments by 40% with no increase in escaped defects.

---

That's it for this week.

Hit reply if you want a review of your team's code review checklist — I'll tell you what to cut and what to automate. I read every response.

– Alex

P.S. For the broader leadership framework on building effective engineering processes: [Engineering Leadership: Founder to CTO](https://alexmayhew.dev/blog/engineering-leadership-founder-to-cto).
